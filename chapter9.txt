# Chapter 9 - Deployments with Ansible

Deploying application code to servers is one of the hardest, but most rewarding, aspects of any development team. Most shops that use traditional deployment techniques (manual steps, shell scripts, and prayers) dread deployments, especially for complex, monolithic apps.

Deployments are less daunting when you adopt modern deployment processes and find the right amount of automation. In the best case, deployments become so boring and routine they barely register as a blip on your team's radar.

Consider Etsy, a company whose engineers are deploying code to production [up to 40 times per day](http://www.slideshare.net/mikebrittain/principles-and-practices-in-continuous-deployment-at-etsy), with no manual intervention from the operations team. The operations team is free to work on more creative endeavors than repetitively completing the same tasks over and over, and the developers get to see their beautifully-crafted code go live in near-real-time!

Etsy's production deployment schedule is enabled by a strong DevOps-oriented culture (with robust code repository management, continuous integration, well-tested code, feature flags, etc.). While it may not be possible to start deploying *your* application to production 20 times a day immediately, you can move a long way towards effortless deployments by automating deployments with Ansible.

## Deployment strategies

There are dozens of ways to deploy code to servers. For the simplest of applications, all that's involved might be switching to a new tag in a code repository on the server and restarting a service.

For more complex applications, you might do a full Blue-Green deployment, where you build an entire new infrastructure alongside your current production infrastructure, run tests on the new infrastructure, then automatically cut over to the new instances. While this may be overkill for many applications (especially if you can tolerate a little deployment downtime), it is becoming more and more common, and Ansible can orchestrate the entire process automatically.

In this chapter, we will be covering the following deployment strategies, using a few different demonstration applications as examples:

  1. Single-server deployments.
  2. Zero-downtime multi-server deployments.
  3. Capistrano-style deployments using `f500.project_deploy`.
  4. Blue-green deployments on AWS.

These are four of the most common deployment techniques, and they cover most common use cases today. There are other ways you can make your deployment processes even more robust, and many application-level and organization-level decisions you can make to ensure deployment success (especially when dealing with schema changes, data migrations, and major feature additions), but those deployment aspects are out of the scope of this book.

## Simple single-server deployments

The vast majority of small applications and websites are easily run on a single virtual machine or dedicated server. Using Ansible to provision and manage the configuration on the server is a no-brainer. Even though you only have to manage *one* server, it's better to encapsulate all the setup so you don't end up with a *snowflake server*.

In this instance, we are managing a very simple Ruby on Rails site that allows users to perform CRUD operations on articles (very simple database records with a title and body).

The code repository for this app is located on GitHub at `https://github.com/geerlingguy/demo-rails-app`.

To make testing simple, we'll begin by creating a new Vagrant VM using the following Vagrantfile:

{lang=ruby}
    # -*- mode: ruby -*-
    # vi: set ft=ruby :
    
    Vagrant.configure(2) do |config|
      config.vm.box = "geerlingguy/ubuntu1404"
    
      config.vm.provider "virtualbox" do |v|
        v.name = "rails-demo"
        v.memory = 1024
        v.cpus = 2
      end
    
      config.vm.hostname = "rails-demo"
      config.vm.network :private_network, ip: "192.168.33.7"
    
      config.vm.provision "ansible" do |ansible|
        ansible.playbook = "playbooks/main.yml"
        ansible.sudo = true
      end
    
    end

In this case, we have a very simple VM that will be accessible at the IP address `192.168.33.7`, and when provisioned, it will run the Ansible playbook defined in `playbooks/main.yml`.

### Provisioning a simple Ruby on Rails server

To prepare for our application deployment, we need to do the following:

  1. Install git (our application is version controlled in a git repository).
  2. Install Node.js (asset compilation requires it's Javascript runtime).
  2. Install Ruby (our application requires version 2.2.0 or later).
  3. Install Passenger with Nginx (we need a fast web server to run our rails application).
  4. Install any other dependencies, and prepare the server for deployment.

To that end, let's create a new playbook just for the provisioning tasks (we'll worry about deployment later), in a new file, `playbooks/provision.yml`:

{lang=text}
    ---
    - hosts: all
      sudo: yes
    
      vars_files:
        - vars.yml
    
      roles:
        - geerlingguy.git
        - geerlingguy.nodejs
        - geerlingguy.ruby
        - geerlingguy.passenger
    
      tasks:
        - name: Install app dependencies.
          apt: "name={{ item }} state=present"
          with_items:
            - libsqlite3-dev
            - libreadline-dev
    
        - name: Ensure app directory exists and is writeable.
          file:
            path: "{{ app_directory }}"
            state: directory
            owner: "{{ app_user }}"
            group: "{{ app_user }}"
            mode: 0755

This is a fairly simple playbook. We'll need to define a few variables to make sure the `geerlingguy.ruby` role installs the correct version of Ruby (at least 2.2.0), and the `geerlingguy.passenger` role is configured to serve our app correctly.

There are also a few other variables we will need, like `app_directory` and `app_user`, so let's create the variables file now, at `playbooks/vars.yml`:

{lang=text}
    # Variables for our app.
    app_directory: /opt/demo-rails-app
    app_user: www-data
    
    # Variables for Passenger and Nginx.
    passenger_server_name: 0.0.0.0
    passenger_app_root: /opt/demo-rails-app/public
    passenger_app_env: production
    passenger_ruby: /usr/local/bin/ruby
    
    # Variables for Ruby installation.
    ruby_install_from_source: true
    ruby_download_url: http://cache.ruby-lang.org/pub/ruby/2.2/ruby-2.2.0.tar.gz
    ruby_version: 2.2.0

The passenger variables tell Passenger to run a server available on every network interface, and to launch our app (which will be located in `/opt/demo-rails-app/public`) with `production` settings (the app's environment), using the `ruby` binary we have installed in `/usr/local/bin/ruby`.

The Ruby variables tell the `ruby` role to install Ruby 2.2.0 from source, since the packages available through Ubuntu's standard apt repositories only contain older versions.

The playbook specified in our Vagrantfile, `playbooks/main.yml`, doesn't yet exist. Let's create that playbook and include the above `provisioning.yml` playbook so our server will be provisioned successfully. We'll separate out the deployment steps into another playbook and include that separately. Inside `playbooks/main.yml`:

{lang=text}
    ---
    - include: provision.yml

### Deploying a Rails app to the server

All the dependencies for our app's deployment were configured in `provision.yml`, so we're ready to build a playbook to perform all the deployment tasks.

Add a line to the `main.yml` file to include a new `deploy.yml` playbook:

{lang=text}
    ---
    - include: provision.yml
    - include: deploy.yml

Now we're ready to create a the `deploy.yml` playbook, which will do the following:

  1. Use git to check out the latest production release of the Rails app.
  2. Copy over a `secrets.yml` template that holds some secure app data required for running the app.
  3. Make sure all the gems required for the app are installed (via Bundler).
  4. Create the database (if it doesn't already exist).
  5. Run `rake` tasks to make sure the database schema is up-to-date and all assets (like JS and CSS) are compiled.
  6. Make sure the app files' ownership is set correctly so Passenger and Nginx can serve them without error.
  7. If any changes or updates were made, restart Passenger and Nginx.

Most of these tasks will use Ansible's modules, but for a few, we'll just wrap the normal deployment-related commands in `shell` since there aren't pre-existing modules to take care of them for us:

{lang=text}
    ---
    - hosts: all
      sudo: yes
    
      vars_files:
        - vars.yml
    
      roles:
        - geerlingguy.passenger
    
      tasks:
        - name: Ensure demo application is at correct release.
          git:
            repo: https://github.com/geerlingguy/demo-rails-app.git
            version: "{{ app_version }}"
            dest: "{{ app_directory }}"
            accept_hostkey: true
          register: app_updated
          notify: restart nginx
    
        - name: Ensure secrets file is present.
          template:
            src: templates/secrets.yml.j2
            dest: "{{ app_directory }}/config/secrets.yml"
            owner: "{{ app_user }}"
            group: "{{ app_user }}"
            mode: 0664
          notify: restart nginx
    
        - name: Install required dependencies with bundler.
          shell: "bundle install --path vendor/bundle chdir={{ app_directory }}"
          when: app_updated.changed == true
          notify: restart nginx
    
        - name: Check if database exists.
          stat: "path={{ app_directory }}/db/{{ app_environment.RAILS_ENV }}.sqlite3"
          register: app_db_exists
    
        - name: Create database.
          shell: "bundle exec rake db:create chdir={{ app_directory }}"
          when: app_db_exists.stat.exists == false
          notify: restart nginx
    
        - name: Perform deployment-related rake tasks.
          shell: "{{ item }} chdir={{ app_directory }}"
          with_items:
            - bundle exec rake db:migrate
            - bundle exec rake assets:precompile
          environment: app_environment
          when: app_updated.changed == true
          notify: restart nginx
    
        - name: Ensure demo application has correct user for files.
          file:
            path: "{{ app_directory }}"
            state: directory
            owner: "{{ app_user }}"
            group: "{{ app_user }}"
            recurse: yes
          notify: restart nginx

The first thing you'll notice (besides the fact that we've included the `vars.yml` file again, since we need those variables in this playbook as well) is that we've added the `geerlingguy.passenger` role in this playbook. Since we'll be using one of the handlers defined in that playbook (`restart nginx`), we need to include the role explicitly. We could've added a separate handler specific to this playbook, but it's more maintainable to reuse handlers from roles if necessary.

Let's walk through the tasks, one-by-one:

  1. (Lines 12-19) We put all the application files in place by checking out the git repository at the version `app_version` into the directory `app_directory`. We set `accept_hostkey` to true so that, the first time we deploy the app, this task doesn't hang since we haven't yet accepted the Git server's hostkey.
  2. (Lines 21-28) We copy a `secrets.yml` file to the application's configuration directory. There are different ways to deploy app secrets, but this is the simplest and easiest, and allows us to store the app secrets in an Ansible Vault-protected vars file if we so desire.
  3. (Lines 30-33) If the `app_updated` variable shows that a change occurred as part of the first `git` task, we'll run a bundler command to ensure all the latest bundled dependencies are installed in the `vendor/bundle` directory.
  4. (Lines 35-42) Create the application database with `rake db:create` if it doesn't already exist. Since this application uses a simple SQLite database, it's a matter of checking if the .sqlite3 file exists, and if not, running the `db:create` task.
  5. (Lines 44-51) If the `app_updated` variable shows that a change occurred as part of the first `git` task, we'll also run a couple `rake` tasks to make sure the database schema is up to date, and all assets (like scripts and stylesheets) are compiled.
  6. (Lines 53-60) Make sure all app files have the correct permissions for Passenger/Nginx to serve them correctly.

Because many of the tasks result in filesystem changes that could change the behavior of the application, they all notify the `restart nginx` handler provided by the `geerlingguy.passenger` role, so Passenger reloads the configuration and restarts the app.

There are a few new variables we need to add to `vars.yml`, and we also need to add the `secrets.yml.j2` template mentioned in the task that copies it into place.

First, we'll create the secrets file, inside `playbooks/templates/secrets.yml.j2`:

{lang=text}
    development:
      secret_key_base: {{ app_secrets.dev }}
    
    test:
      secret_key_base: {{ app_secrets.test }}
    
    production:
      secret_key_base: {{ app_secrets.prod }}

We'll be using a dictionary variable for `app_secrets`, so let's add that and all the other new variables to `playbooks/vars.yml`:

{lang=text}
    ---
    # Variables for our app.
    app_version: 1.2.2
    app_directory: /opt/demo-rails-app
    app_user: www-data
    app_secrets:
      dev: fe562ec1e21eecc5af4d83f6a157a7
      test: 4408f36dd290766d2f368fdfcedf4d
      prod: 9bf801da1a24c9a103ea86a1438caa
    app_environment:
      RAILS_ENV: production

    # Variables for Passenger and Nginx.
    passenger_server_name: 0.0.0.0
    passenger_app_root: /opt/demo-rails-app/public
    passenger_app_env: production
    passenger_ruby: /usr/local/bin/ruby

    # Variables for Ruby installation.
    ruby_install_from_source: true
    ruby_download_url: http://cache.ruby-lang.org/pub/ruby/2.2/ruby-2.2.0.tar.gz
    ruby_version: 2.2.0

Note the addition of the following variables to support our `deploy.yml` playbook:

  - `app_version`: This is the git tag or branch tip to be deployed to the server.
  - `app_secrets`: A dictionary of Rails app secrets, which are used to verify the integrity of signed app cookies. You can generate new, unique strings for these variables using `rake secret`.
  - `app_environment`: Environment settings required for certain commands (like `bundle exec` and `rake`) to run with the correct Rails application environment.

### Provisioning and Deploying the Rails App

Since we now have our `provision.yml` and `deploy.yml` playbooks completed, and both are `include`d in the `main.yml` playbook Vagrant will run, we can finally bring up the new VM using Vagrant, and see if our application works!

The structure of your project folder should look like this:

{lang=text,linenos=off}
    deployments/
      playbooks/
        templates/
          secrets.yml.j2
        deploy.yml
        main.yml
        provision.yml
        vars.yml
      Vagrantfile

Before we can run the playbook, we need to make sure all the role dependencies are present. If you were building everything from scratch, you might have a `roles` directory with all the roles inside, but in this case, since we're using roles from Ansible Galaxy, it's best to not include the role files directly with our playbook, but instead, add a `requirements.txt` file to the project and install the roles automatically with Galaxy.

Inside `requirements.txt`:

{lang=text}
    geerlingguy.git
    geerlingguy.ruby
    geerlingguy.nodejs
    geerlingguy.passenger

Now, in the same directory as that file, run the command `$ ansible-galaxy install -r requirements.txt`, and after a minute, all the required roles will be downloaded to your default Ansible roles directory, if they're not already present.

Change directory back to the main directory containing the `Vagrantfile`, and run `vagrant up`. Assuming everything runs correctly, you should see the playbook complete successfully after a few minutes:

{lang=text,linenos=off}
    TASK: [Ensure demo application has correct user for files.] *************
    changed: [default]
    
    NOTIFIED: [geerlingguy.passenger | restart nginx] ***********************
    changed: [default]
    
    PLAY RECAP **************************************************************
    default               : ok=46   changed=28   unreachable=0    failed=0

Now, jump over to a web browser and load `http://192.168.33.7/`. You should see something like the following:

{width=80%}
![Demonstration Rails app running successfully.](images/9-rails-app-fresh.png)

Try creating, updating, and deleting a few articles to make sure the database and all app functionality is working correctly:

{width=80%}
![A simple app to perform CRUD operations on Articles.](images/9-rails-app-with-articles.png)

The app seems to function perfectly, but it could use some improvements. After more development work, we have a new version of to deploy. We could update the `app_version` variable in `vars.yml` and run `vagrant provision` to run the entire provisioning and deployment playbook again, but to save a little time, and to utilize the more flexible playbook layout (with provisioning and deployment concerns separated), we can run the `deploy.yml` playbook separately.

### Deploying application updates

First, to test whether we can deploy without provisioning, we will need to create an inventory file to tell Ansible how to connect directly to the Vagrant-managed VM.

Create the file `playbooks/inventory-ansible` with the following contents:

{lang=text}
    [rails]
    192.168.33.7
    
    [rails:vars]
    ansible_ssh_user=vagrant
    ansible_ssh_private_key_file=~/.vagrant.d/insecure_private_key

T> If you were creating this playbook for a server or VM running outside of Vagrant's control, you'd probably have already created an inventory file or added the server to your global inventory, but when we're working with Vagrant, it's often convenient to use Vagrant's own dynamically-managed inventory. Running playbooks outside of Vagrant's `up`/`provision` functionality requires us to create a separate inventory file.

Test the ability to run the `deploy.yml` playbook by running the following command inside the `playbooks` directory:

{lang=text,linenos=off}
    $ ansible-playbook deploy.yml -i inventory-ansible

Hopefully the playbook completed its run successfully. It may have reported a change in the "Ensure demo application has correct user for files" task, and if so, it will have restarted Passenger. Run it again, and ansible should report no changes:

{lang=text,linenos=off}
    PLAY RECAP **************************************************************
    192.168.33.7          : ok=16   changed=0    unreachable=0    failed=0

Hopefully you've noticed that running the `deploy.yml` playbook standalone is much faster than running the `provision` and `deploy` playbooks together (deployment only takes 16 tasks, while both playbooks add up to 70+ tasks!). In the future, we can deploy application updates using only the `deploy.yml` playbook and changing the `app_version` either in `vars.yml` or by specifying the version on the command line in the `ansible-playbook` command.

T> It's generally preferred to change variables in vars files that are versioned with your playbooks, rather than specify them through inventory files, environment variables, or on the command line. This way the entire state of your infrastructure is encapsulated in your playbook files, which ideally should be version controlled and managed similarly to the application they deploy. Plus, who wants to enter any more information on the command line than is absolutely required?

Our application is a fairly generic web application that has updates to application code (which require a webserver reload), styles (which need recompiling), and possibly the database schema (which needs `rake` migrate tasks to be run). Any time `app_version` is changed inside `playbooks/vars.yml`, the deploy playbook will automatically run all the required tasks to get our app running with the latest code.

Update `app_version` to `1.3.0`, and then run the following command again:

{lang=text,linenos=off}
    $ ansible-playbook deploy.yml -i inventory-ansible

After a minute or so, the deployment should complete, and once that's done, you'll see the much improved new version of the Demonstration Ruby on Rails Application:

{width=80%}
![Rails app - version 1.3.0 with a responsive UI.](images/9-rails-app-1-3-0.png)

Simple application update deployments will involve incrementing the `app_version` to the latest git tag, then running the `deploy.yml` playbook again. You can always run the `main.yml` playbook to ensure the entire server stack is in the correct state, but it's faster to just deploy the app updates.

## Zero-downtime multi-server deployments

A single server deployment strategy is all that's needed for many applications, but if you need to run an application on multiple servers for horizontal scalability or redundancy, deployments can be cumbersome---but not when you use Ansible!

[Server Check.in](https://servercheck.in/) is a simple server and website monitoring service that has a microservices-based architecture; there is a website, an API application, and a server checking application.

The server checking application needs to run on a variety of servers hosted around the world by different providers to provide redundancy and reliability. Server Check.in uses Ansible to manage *rolling deployments* for this application, so new code can be deployed across all the servers in minutes while maintaining 100% uptime!

We'll emulate part of Server Check.in's infrastructure (the check server application) by deploying and updating a simple Node.js application to a set of virtual machines. The code repository for this app is located on GitHub at `https://github.com/geerlingguy/demo-nodejs-api`. Here's a diagram of the infrastructure we'll be building:

TODO - IMAGE: four servers connected to cloud

To begin, create four lightweight Vagrant VMs using the following Vagrantfile:

{lang=ruby}
    # -*- mode: ruby -*-
    # vi: set ft=ruby :
    
    Vagrant.configure("2") do |config|
      # Base VM OS configuration.
      config.vm.box = "geerlingguy/ubuntu1404"
      config.vm.synced_folder '.', '/vagrant', disabled: true
      config.ssh.insert_key = false
    
      config.vm.provider :virtualbox do |v|
        v.memory = 256
        v.cpus = 1
      end
    
      # Define four VMs with static private IP addresses.
      boxes = [
        { :name => "nodejs1", :ip => "192.168.3.2" },
        { :name => "nodejs2", :ip => "192.168.3.3" },
        { :name => "nodejs3", :ip => "192.168.3.4" },
        { :name => "nodejs4", :ip => "192.168.3.5" }
      ]
    
      # Provision each of the VMs.
      boxes.each do |opts|
        config.vm.define opts[:name] do |config|
          config.vm.hostname = opts[:name]
          config.vm.network :private_network, ip: opts[:ip]
    
          # Provision all the VMs using Ansible after the last VM is booted.
          if opts[:name] == "nodejs4"
            config.vm.provision "ansible" do |ansible|
              ansible.playbook = "playbooks/main.yml"
              ansible.inventory_path = "inventory"
              ansible.limit = "all"
            end
          end
        end
      end
    
    end

The above `Vagrantfile` defines four VMs that each use 256MB of RAM and have a unique hostname and IP address (defined by the `boxes` variable). Our Node.js app doesn't require much in the way of processing power or memory.

In the `provision` section of the playbook, we told Vagrant to provision the all the VMs with Ansible, using the inventory file `inventory`, and the playbook `playbooks/main.yml`. Create these two files in the same folder as your Vagrantfile:

{lang=text,linenos=off}
    rolling-deployments/
      playbooks/
        main.yml
      inventory
      Vagrantfile

Inside the `inventory` file, we just need to define a list of all the Node.js API app VMs by IP address:

{lang=text}
    [nodejs-api]
    192.168.3.2
    192.168.3.3
    192.168.3.4
    192.168.3.5
    
    [nodejs-api:vars]
    ansible_ssh_user=vagrant
    ansible_ssh_private_key_file=~/.vagrant.d/insecure_private_key

Inside the `main.yml` playbook, we'll call out two separate playbooks---one for the initial provisioning (installing Node.js and making sure the server is configured correctly), and another for deployment (ensuring our Node.js API app is present and running):

{lang=text}
    ---
    - include: provision.yml
    - include: deploy.yml

Go ahead and create the `provision.yml` and `deploy.yml` playbooks, starting with `provision.yml`:

{lang=text}
    ---
    - hosts: nodejs-api
      sudo: yes
    
      vars:
        nodejs_forever: true
        firewall_allowed_tcp_ports:
          - "22"
          - "8080"
    
      roles:
        - geerlingguy.firewall
        - geerlingguy.nodejs

This extremely simple playbook runs on all the servers defined in our inventory file, and runs two roles on the servers: `geerlingguy.firewall` (which installs and configures a firewall, in this case opening ports 22 for SSH and 8080 for our app) and `geerlingguy.nodejs` (which installs Node.js, NPM, and `forever`, which we'll use to run our app as a daemon).

Since we're using two roles from Ansible Galaxy, it's best practice to also include those roles in a requirements file so CI tools and others using this playbook can easily install all the required roles.

Create a `requirements.txt` file in the root folder and add the following:

{lang=text}
    geerlingguy.firewall
    geerlingguy.nodejs

Now, whenever someone new to the project wants to run the playbook, all that person needs to do is run `ansible-galaxy install -r requirements.txt` to install all the required roles.

At this point, your project directory should be structured like the following:

{lang=text,linenos=off}
    rolling-deployments/
      playbooks/
        deploy.yml
        main.yml
        provision.yml
      inventory
      requirements.txt
      Vagrantfile

Before we can run `vagrant up` and see our infrastructure in action, we need to build out the `deploy.yml` playbook, which will ensure our app is present and running correctly on all the servers.

Inside `deploy.yml`, add the following:

{lang=text}
    ---
    - hosts: nodejs-api
      gather_facts: no
      sudo: yes
    
      vars_files:
        - vars.yml

Use `sudo` for this playbook to keep things simple, and set `gather_facts` to `no` to save a little time during deployments, since our simple app doesn't require any of the gathered system facts to run.

Since we have a few variables to define, and we'd like to track them separately for easier file revision history, we'll define the variables in a `vars.yml` file in the same directory as the `deploy.yml` playbook:

{lang=text}
    ---
    app_repository: https://github.com/geerlingguy/demo-nodejs-api.git
    app_version: "1.0.0"
    app_directory: /opt/demo-nodejs-api

Once you've saved the `vars.yml` file, continue building out `deploy.yml`, starting with a task to clone the app's repository (which we just defined in `vars.yml`):

{lang=text,starting-line-number=9}
      tasks:
        - name: Ensure Node.js API app is present.
          git:
            repo: "{{ app_repository }}"
            version: "{{ app_version }}"
            dest: "{{ app_directory }}"
            accept_hostkey: true
          register: app_updated
          notify: restart forever apps

Using variables for the `git` module's `repo` and `version` affords flexibility; app version changes might happen frequently, and it's easier to manage that in a separate `vars.yml` file.

We also want to `notify` a `restart forever apps` handler whenever the codebase is changed. We'll define the `restart forever apps` handler later in the playbook.

{lang=text,starting-line-number=18}
        - name: Stop all running instances of the app.
          command: "forever stopall"
          when: app_updated.changed
    
        - name: Ensure Node.js API app dependencies are present.
          npm: "path={{ app_directory }}"
          when: app_updated.changed
    
        - name: Run Node.js API app tests.
          command: "npm test chdir={{ app_directory }}"
          when: app_updated.changed

Once the app is present on the server, we need to use `npm` to install dependencies (using Ansible's `npm` module), then run the app's test suite using `npm test`. To save time, we only stop the application, update dependencies, and run tests if the application has changed (using the `app_updated` variable we registered when checking out the application code).

Running the tests for the app during every deployment ensures the app is present and in a functioning state. Having a thorough unit and integration test suite that runs on every deployment is almost prerequisite to a frequent or continuously-integrated project! Running the tests during deployments also helps with ensuring zero-downtime deployments, as we'll see later.

{lang=text,starting-line-number=25}
        - name: Get list of all running Node.js apps.
          command: forever list
          register: forever_list
          changed_when: false
    
        - name: Ensure Node.js API app is started.
          command: "forever start {{ app_directory }}/app.js"
          when: "forever_list.stdout.find('app.js') == -1"

Once the app is present and running correctly, we need to make sure it's started. There's a command to get the list of all running apps (using `forever`), then a command to start the app if it's not already running.

{lang=text,starting-line-number=34}
        - name: Add cron entry to start Node.js API app on reboot.
          cron:
            name: "Start Node.js API app"
            special_time: reboot
            job: "forever start {{ app_directory }}/app.js"

The final task adds a cron job to make sure the app is started after the server reboots. Since we're managing the deamonization of our app using `forever` instead of the OS's init system, it's simplest to make sure the app starts on system boot using a `reboot` cron job.

Remember when we added the line `notify: restart forever apps` to the task that ensured the app was present on the server? It's time to define that handler, which runs the command `forever restartall` (which does exactly what it says):

{lang=text,starting-line-number=40}
      handlers:
        - name: restart forever apps
          command: "forever restartall"

At this point, the Ansible playbooks and Vagrant configuration should be complete. The playbook will clone the `demo-nodejs-api` project, run its tests to make sure everything's working correctly, then start the app using `forever` and make sure it's started whenever the the server reboots.

You can run the command below to test all the new servers and make sure the app is running correctly:

{lang=text,linenos=off}
    $ for i in {2..5}; \
        do curl -w "\n" "http://192.168.3.$i:8080/hello/john"; \
      done

If all the servers are online, you should see the text `"hello john"` repeated four times (once for each server):

{lang=text,linenos=off}
    "hello john"
    "hello john"
    "hello john"
    "hello john"

You can run `vagrant provision` to run the entire provisioning and deployment process again, or just run `ansible-playbook -i inventory playbooks/deploy.yml` to run the deployment playbook again. In either case, you should see no changes, and Ansible should verify that everything's `ok`.

You now have a fleet of Node.js API servers similar to Server Check.in's server checking infrastructure---except it doesn't do much yet! Luckily, the project has seen some new feature development since the initial `1.0.0` version you just deployed. We now need a way to get the new version deployed to and running on all the servers while maintaining 100% uptime for the API as a whole.

### Ensuring zero downtime with `serial` and integration tests

`ansible-playbook -i inventory playbooks/deploy.yml`

TODO.

### Deploying to app servers behind a load balancer

In the case of Server Check.in, there are two separate API layers that manage the complexity of ensuring all server checks happen, regardless of whether certain servers are up or down. The 'load balancing' occurs on the application layer instead of as a separate infrastructure layer (this is extremely helpful when dealing with global latency and network reliability variation).

For many applications, especially those with app servers close together (e.g. in the same data center) the infrastructure layer follows a more traditional layout, with a load balancer to handle the API request distribution:

TODO - IMAGE: many app servers behind one LB

TODO.

## Capistrano-style deployments

TODO:

  - [Deploying with Ansible](https://groups.google.com/forum/?#!topic/ansible-project/R3Kr2uMYUt4)
  - [project_deploy module](https://galaxy.ansible.com/list#/roles/2266) (something similar is slated for core inclusion)

## Blue-green deployments

TODO:

  - Set up parallel infrastructure.
  - Run tests on new infrastructure.
  - Manage load balancers (BigIP, ELB, netscaler, etc.) in [pre|post]_task.
  - After cutover, destroy old infrastructure.

## Summary

TODO:

  - `max_fail_percentage`
  - `run_once` (e.g. for a database update command)
  - `delegate_to` / `local_action`
  - [Notifications with Ansible](http://www.ansible.com/blog/listen-to-your-servers-talk)

{lang=text,linenos=off}
     _______________________________________
    / One machine can do the work of fifty  \
    | ordinary men. No machine can do the   |
    | work of one extraordinary man.        |
    \ (Elbert Hubbard)                      /
     ---------------------------------------
            \   ^__^
             \  (oo)\_______
                (__)\       )\/\
                    ||----w |
                    ||     ||
