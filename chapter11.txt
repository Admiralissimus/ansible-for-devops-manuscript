# Chapter 11 - Automating Your Automation - Ansible Tower and CI/CD

At this point, you should be able to convert almost any bit of your infrastructure's configuration into Ansible playbooks, roles, and inventories. And before deploying any infrastructure changes, you should test the changes in a non-production environment (just like you would with application releases). Manually running a playbook that configures your entire infrastructure, then making sure it does what you expect, is a good start towards order and stability.

Since all your infrastructure is defined in code, you can start automating all the aspects of infrastructure deployment, and even run unit, functional, and integration tests on your infrastructure, just like you do for your applications.

This section will cover different levels of infrastructure automation and testing, and highlight tools and techniques you can use to automate and streamline infrastructure operations.

## Ansible Tower

Throughout this book, all the examples use Ansible's CLI to run playbooks and report back the results. For smaller teams, especially when everyone on the team is well-versed in how to use Ansible, YAML syntax, and follows security best practices with playbooks and variables files, using the CLI can be a sustainable approach.

But for many organizations, there are needs that stretch basic CLI use too far:

  - The business needs detailed reporting of infrastructure deployments and failures, especially for audit purposes.
  - Team-based infrastructure management requires varying levels of involvement in playbook management, inventory management, and key and password access.
  - A thorough visual overview of the current and historical playbook runs and server health helps identify potential issues before they affect the bottom line.
  - Playbook scheduling can help ensure infrastructure remains in a known state.

Ansible Tower checks off these items---and many more---and provides a great mechanism for team-based Ansible usage. The product is currently free for teams managing ten or fewer servers (it's basically an 'unlimited trial' mode), and has flexible pricing for teams managing dozens to thousands of servers.

While this book includes a brief overview of Tower, and how you can get started with Tower, it is highly recommended that you read through Ansible, Inc's extensive [Tower User Guide](http://releases.ansible.com/ansible-tower/docs/tower_user_guide-latest.pdf), which includes details this book won't be covering such as LDAP integration and multiple-team playbook management workflows.

### Getting and Installing Ansible Tower

Ansible has a very thorough [Ansible Tower User Guide](http://releases.ansible.com/ansible-tower/docs/tower_user_guide-latest.pdf), which details the installation and configuration of Ansible Tower. For the purposes of this chapter, since we just want to download and try out Tower locally, we are going to use Ansible's official Vagrant box to quickly build an Ansible Tower VM.

Make sure you have [Vagrant](https://www.vagrantup.com/downloads.html) and [VirtualBox](https://www.virtualbox.org/wiki/Downloads) installed, then create a directory (e.g. `tower`) and do the following within the directory:

  1. `vagrant init tower http://vms.ansible.com/ansible-tower-2.1.4-virtualbox.box` (Create a new Vagrantfile using the Tower base box from Ansible).
  2. `vagrant up` (Build the Tower VM).
  3. `vagrant ssh` (Log into the VM, and Tower will display a message with connection information).

I> The above installation instructions and Vagrant box come from a blog post on Ansible's official blog, [Ansible Tower and Vagrant](http://www.ansible.com/blog/ansible-vagrant).

You can now visit the URL provided by the login welcome message (something like `https://10.42.0.42/`), and after confirming a security exception for the Ansible Tower certificate, login with the credentials from step 3.

At this point, you will need to register a free trial license of Ansible Tower, which can be done following the instructions on the screen. The free trial allows you to use all of Tower's features for up to 10 servers, and is great for experimenting and seeing how Tower fits into your workflow. After you get the license (it's a block of JSON which you paste into the license field), you should get to Tower's default dashboard page:

{width=80%}
![Ansible Tower's Dashboard](images/11-ansible-tower-dashboard.png)

### Using Ansible Tower

Ansible Tower is centered around the idea of organizing *Projects* (which run your playbooks via *Jobs*) and *Inventories* (which describe the servers on which your playbooks should be run) inside of *Organizations*. *Organizations* can then be set up with different levels of access based on *Users* and *Credentials* grouped in different *Teams*. It can be a little overwhelming at first, but once you get the initial structure configured, you'll see how powerful and flexible Tower's Project workflow is.

Let's get started with our first project!

The first step is to make sure you have a test playbook you can run using Ansible Tower. Generally, your playbooks should be stored in a source code repository (e.g. Git or Subversion), with Tower configured to check out the latest version of the playbook from the repository and run it. Since we're only going to run a simple example, we will create a playbook in Tower's default `projects` directory located in `/var/lib/awx/projects`:

  1. Log into the Tower VM: `vagrant ssh`
  2. Switch to the `awx` user: `sudo su - awx`
  3. Go to Tower's default `projects` directory: `cd /var/lib/awx/projects`
  4. Create a new project directory: `mkdir ansible-for-devops && cd ansible-for-devops`
  5. Create a new playbook file, `main.yml`, within the new directory, with the following contents:

{lang=text}
    ---
    - hosts: all
      gather_facts: no
      connection: local
    
      tasks:
        - name: Check the date on the server.
          command: date

Switch back to your web browser and get everything set up to run the test playbook inside Ansible Tower's web UI:

  1. Create a new *Organization*, called 'Ansible for DevOps'.
  2. Add a new User to the Organization, named John Doe, with the username `johndoe` and password `johndoe1234`.
  3. Create a new *Team*, called 'DevOps Engineers', in the 'Ansible for DevOps' Organization.
  4. Under the Team's Credentials section, add in SSH credentials by selecting 'Machine' for the Credential type, and setting 'Name' to `Vagrant`, 'Type' to `Machine`, 'SSH Username' to `vagrant`, and 'SSH Password' to `vagrant`.
  5. Under the Team's Projects section, add a new *Project*. Set the 'Name' to `Tower Test`, 'Organization' to `Ansible for DevOps`, 'SCM Type' to `Manual`, and 'Playbook Directory' to `ansible-for-devops` (Tower automatically detects all folders placed inside `/var/lib/awx/projects`, but you could also use an alternate Project Base Path if you want to store projects elsewhere).
  6. Under the Inventories section, add an *Inventory*. Set the 'Name' to `Tower Local`, and 'Organization' set to `Ansible for DevOps`. Once the inventory is saved:
    1. Add a 'Group' with the Name `localhost`. Click on the group once it's saved.
    2. Add a 'Host' with the Host Name `127.0.0.1`.

T> New *Credentials* have a somewhat dizzying array of options, and offer login and API key support for a variety of services, like SSH, AWS, Rackspace, VMWare vCenter, and SCM systems. If you can login to a system, Tower likely supports the login mechanism!

Now that we have all the structure for running playbooks configured, we need only create a *Job Template* so we can run the playbook on the localhost and see whether we've succeeded. Click on 'Job Templates', and create a new Job Template with the following configuration:

  - Name: `Tower Test`
  - Inventory: `Tower Local`
  - Project: `Tower Test`
  - Playbook: `main.yml`
  - Machine Credential: `Vagrant`

Save the Job Template, then click the small Rocketship button to start a job using the template. You'll be redirected to a Job status page, which provides live updates of the job status, and then a summary of the playbook run when complete:

{width=80%}
![Tower Test job completed successfully!](images/11-ansible-tower-job-complete.png)

You can view the playbook run's standard output in real-time (or review it after the fact) with the 'View standard out' button. You can also stop a running job, delete a job's record, or relaunch a job with the same parameters using the respective buttons on the job's page.

The job's dashboard page is very useful for giving an overview of how many hosts were successful, how many tasks resulted in changes, and the timing of the different parts of the playbook run.

### Other Tower Features of Note

In our simple walkthrough above, we used Tower to run a simple playbook on the local server; setting up Tower to run playbooks on real-world infastructure or other local VMs is just as easy, and the tools Ansible Tower provides are very handy, especially when working in larger team environments.

This book won't walk through the entirety of Ansible Tower's documentation, but a few other simple features you should try out include:

  - Setting up scheduled Job runs (especially with the 'Check' option instead of 'Run') for CI/CD.
  - Integrating user accounts and Teams with LDAP users and groups for automatic team-based project management.
  - Setting different levels of permissions for Users and Teams so certain users can edit certain jobs, others can only run certain jobs, and yet others can only view the results of job runs within an Organization.
  - Configuring Ansible Vault credentials so you can easily and automatically use Vault-protected variables in your playbooks.
  - Setting up Provisioning Callbacks so newly-provisioned servers can self-provision via a URL per Job Template.
  - Surveys, which allow users to add extra information based on a 'Survey' of questions per job run.
  - Inventory Scripts, which allow you to build inventory dynamically.
  - Built-in Munin monitoring (to monitor the Tower server), available with the same admin credentials at `https://[tower-hostname]/munin`.

Ansible Tower continues to improve rapidly, and is one of the best ways to run Ansible Playbooks from a central CI/CD-style server with team-based access and extremely detailed live and historical status reporting.

### Tower Alternatives

Ansible Tower is purpose-built for use with Ansible playbooks, but there are many other ways to run playbooks on your servers with a solid workflow. If price is a major concern, and you don't need all the bells and whistles Tower provides, you can use other popular tools like [Jenkins](http://jenkins-ci.org/), [Rundeck](http://rundeck.org/), or [Go CI](http://www.go.cd/).

All these tools provide flexiblity and security for running Ansible Playbooks, and each one requires a different amount of setup and configuration before it will work well for common usage scenarios. One of the most popular and long-standing CI tools is Jenkins, so we'll explore how to configure a similar Playbook run in Jenkins next.

## Jenkins CI

Jenkins is a Java-based open source continuous integration tool. It was forked from the Hudson project in 2011, but has a long history as a robust build tool for most any software project.

Jenkins is easy to install and configure, with the Java SDK as its only requirement. You can install Jenkins on any modern OS, but for the purposes of this demonstration, we'll build a local VM using Vagrant, install Jenkins inside the VM using Ansible, then use Jenkins to run an Ansible playbook.

### Build a local Jenkins server with Ansible

Create a new directory for the Jenkins VM named `jenkins`. Inside the directory, create a `Vagrantfile` to describe the machine and the Ansible provisioning to Vagrant, with the following contents:

{lang=ruby}
    VAGRANTFILE_API_VERSION = "2"
    
    Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
      config.vm.box = "geerlingguy/centos7"
      config.vm.hostname = "jenkins.dev"
      config.vm.network :private_network, ip: "192.168.76.76"
      config.ssh.insert_key = false
    
      config.vm.provider :virtualbox do |vb|
        vb.customize ["modifyvm", :id, "--memory", "512"]
      end
    
      # Ansible provisioning.
      config.vm.provision "ansible" do |ansible|
        ansible.playbook = "provision.yml"
        ansible.sudo = true
      end
    end

This Vagrantfile will create a new VM running CentOS 7, with the IP address `192.168.76.76` and the hostname `jenkins.dev`. Go ahead and add an entry for `192.168.76.76  jenkins.dev` to your hosts file, and then create a new `provision.yml` playbook so Vagrant can run it with Ansible (as described in the `config.vm.provision` block in the Vagrantfile). Put the following in the `provision.yml` file:

{lang=text}
    ---
    - hosts: all
    
      vars:
        firewall_allowed_tcp_ports:
          - "22"
          - "8080"
        jenkins_plugins:
          - ansicolor
    
      roles:
        - geerlingguy.firewall
        - geerlingguy.ansible
        - geerlingguy.java
        - geerlingguy.jenkins

This playbook uses a set of roles from Ansible Galaxy to install all the required components for our Jenkins CI server. To make sure you have all the required roles installed on your host machine, add a `requirements.txt` file in the `jenkins` folder, containing all the roles being used in the playbook:

{lang=text}
    geerlingguy.firewall
    geerlingguy.ansible
    geerlingguy.java
    geerlingguy.jenkins

The `geerlingguy.ansible` role installs Ansible on the VM, so Jenkins can run Ansible playbooks and ad-hoc commands. The `geerlingguy.java` role is a dependency of `geerlingguy.jenkins`, and the `geerlingguy.firewall` role configures a simple firewall to limit access on ports besides 22 (for SSH) and 8080 (Jenkins' default port).

Finally, we tell the `geerlingguy.jenkins` role a set of plugins to install through the `jenkins_plugins` variable; in this case, we just want the `ansicolor` plugin, which gives us full color display in Jenkins' console logs (so our Ansible playbook output is easier to read).

T> There is an official [Ansible plugin for Jenkins](https://wiki.jenkins-ci.org/display/JENKINS/Ansible+Plugin), which can be used to run Ansible Ad-Hoc tasks and Playbooks, but as of the writing of this book, the plugin was still in early development stages and is not yet the recommended way to run Ansible playbooks via Jenkins.

To build the VM and run the playbook, do the following (inside the `jenkins` folder):

  1. Run `ansible-galaxy install -r requirements.txt` to install the required roles.
  2. Run `vagrant up` to build the VM and install and configure Jenkins.

After a few minutes, the provisioning should complete, and you should be able to access Jenkins at `http://jenkins.dev:8080/` (if you configured the hostname in your hosts file).

### Create an Ansible playbook on the Jenkins server

It's preferred to keep your playbooks and server configuration in a code repository (e.g. Git or SVN), but for simplicity's sake, this example requires a playbook stored locally on the Jenkins server, similar to the earlier Ansible Tower example.

  1. Log into the Jenkins VM: `vagrant ssh`
  2. Go to the `/opt` directory: `cd /opt`
  3. Create a new project directory: `sudo mkdir ansible-for-devops && cd ansible-for-devops`
  4. Create a new playbook file, `main.yml`, within the new directory, with the following contents (use sudo to create the file, e.g. `sudo vi main.yml`):

    {lang=text}
    ~~~
    ---
    - hosts: 127.0.0.1
      gather_facts: no
      connection: local
    
      tasks:
        - name: Check the date on the server.
          command: date
    ~~~

After you create the playbook, you will head over to the Jenkins UI to create a job to run the playbook. But if you want, you can quickly test the playbook while you're logged in, with `ansible-playbook main.yml`.

### Create a Jenkins job to run an Ansible Playbook

Now that you have Jenkins running, you can configure a Jenkins job to run a playbook on the local server with Ansible. Visit `http://jenkins.dev:8080/`, and once the page loads, click the 'New Item' link to create a new 'Freestyle project' with a title 'ansible-local-test'. Click 'OK' and when configuring the job, and set the following configuration:

  - Under 'Build Environment', check the 'Color ANSI Console Output' option. This allows Ansible's helpful colored output to pass through the Jenkins console, so it is easier to read during and after the run.
  - Under 'Build', click 'Add Build Step', then choose 'Execute shell'. In the 'Command' field that appears, add the following code, which will run the local Ansible playbook:

    {lang=text}
    ~~~
    # Force Ansible to output jobs with color.
    export ANSIBLE_FORCE_COLOR=true
    
    # Run the local test playbook.
    ansible-playbook /opt/ansible-for-devops/main.yml
    ~~~

Click 'Save' to save the 'Ansible Local Test' job, and on the project's page, click the 'Build Now' link to start a build. After a few seconds, you should see a new item in the 'Build History' block. Click on the (hopefully) blue circle to the left of '#1', and it will take you to the console output of the job. It should look something like this:

{width=80%}
![Jenkins job completed successfully!](images/11-jenkins-job-console-output.png)

This is an extremely simple example, but hopefully it's enough to show you how easy it is to get at least some of your baseline CI/CD automation done using a free and open source tool. Most of the more difficult aspects of managing infrastructure through Jenkins surrounds the ability to manage SSH keys, certificates, and other credentials through Jenkins, but there is already plenty of documentation surrounding these things elsewhere online and in Jenkins documentation, so this will be left as an exercise for the reader.

The rest of this chapter focuses on ways to test and debug your playbooks and your infrastructure as a whole, and while many examples use Travis CI or plain command line options, anything you see can be automated with Jenkins jobs!

## Unit, Integration, and Functional Testing

When determining how you should test your infrastructure, you need to understand the different kinds of testing, and then determine the kinds of testing on which you should focus more effort.

*Unit* testing, when applied to applications, is testing that applies to the smallest units of code (usually functions or class methods). In Ansible, unit testing would typically apply to individual playbooks. You could run individual playbooks in an isolated environment, but that's often not worth the effort. What *is* worth your effort is at least checking the playbook syntax, to make sure you didn't just commit a YAML file that will break an entire deployment because of a missing quotation mark, or a whitespace issue!

*Integration* testing, which is definitely more valuable when it comes to Ansible, is the testing of small groupings of individual units of code, to make sure they work correctly together. Breaking your infrastructure definition into many task-specific roles and playbooks allows you to do this; if you've structured your playbooks so they have no or limited dependencies, you could test each role individually in a fresh virtual machine, before you use the role as part of a full infrastructure deployment.

*Functional* testing involves the whole shebang. Basically, you set up a complete infrastructure environment, and then run tests against it to make sure *everything* was successfully installed, deployed, and configured. Ansible's own reporting is helpful in this kind of testing (and often, all that is necessary), and there are also external tools that can be used to test infrastructure even more deeply.

It is often possible to perform all the testing you need on your own local workstation, using Virtual Machines (as demonstrated in earlier chapters), using tools like VirtualBox or VMWare. And with most cloud services providing robust control APIs and hourly billing, it's often simple, inexpensive, and just as fast to test directly on cloud instances that mirror your production infrastructure!

We'll begin with some of the simplest tests you can run using Ansible, along with some common debugging techniques, then progress to full-fledged functional testing methods with an automated process.

### Debugging and Asserting

For most playbooks, simply testing configuration changes and the result of commands being run as you go is all the testing you need. And having tests run *during your playbook runs* using some of Ansible's built-in utility modules means you have immediate assurance the system is in the state you want.

If at all possible, you should try to bake all simple test cases (e.g. simple comparison and state checks) into your playbooks directly. Ansible has three modules that simplify this process.

#### The `debug` module

When actively developing an Ansible playbook, or even for historical logging purposes (e.g. if you're running Ansible playbooks using Tower or another CI system), it's often handy to print values of variables or output of certain commands during the playbook run.

For this purpose, Ansible has a `debug` module, which prints variables or messages during playbook execution.

As an extremely basic example, here are two of the ways I normally use debug while building a playbook:

{lang=text}
    - hosts: 127.0.0.1
      gather_facts: no
      connection: local
    
      tasks:
        - name: Register the output of the 'uptime' command.
          command: uptime
          register: system_uptime
    
        - name: Print the registered output of the 'uptime' command.
          debug: var=system_uptime.stdout
    
        - name: Print a simple message if a command resulted in a change.
          debug: msg="Command resulted in a change!"
          when: system_uptime.changed

Running this playbook gives the following output:

{lang=text,linenos=off}
    $ ansible-playbook debug.yml
    
    PLAY [127.0.0.1] ******************************************************
    
    TASK: [Register the output of the 'uptime' command.] ******************
    changed: [127.0.0.1]
    
    TASK: [Print the registered output of the 'uptime' command.] **********
    ok: [127.0.0.1] => {
        "var": {
            "system_uptime.stdout":
                "15:01  up 15:18, 2 users, load averages: 1.23 1.33 1.42"
        }
    }
    
    TASK: [Print a simple message if a command resulted in a change.] *****
    ok: [127.0.0.1] => {
        "msg": "Command resulted in a change!"
    }
    
    PLAY RECAP ************************************************************
    127.0.0.1          : ok=3    changed=1    unreachable=0    failed=0

Debug messages are helpful when actively debugging a playbook or when you need extra verbosity in the playbook's output, but if you need to perform an explicit test on some variable, or bail out of a playbook for some reason, Ansible provides the `fail` module, and its even simpler cousin, `assert`.

#### The `fail` and `assert` modules

Both `fail` and `assert`, when triggered, will abort the playbook run, and the only difference is in the simplicity of their usage. To illustrate, let's look at an example:

{lang=text}
    - hosts: 127.0.0.1
      gather_facts: no
      connection: local
    
      vars:
        should_fail_via_fail: true
        should_fail_via_assert: false
        should_fail_via_complex_assert: false
    
      tasks:
        - name: Fail if conditions warrant a failure.
          fail: msg="There was an epic failure."
          when: should_fail_via_fail
    
        - name: Stop playbook if an assertion isn't validated.
          assert: that="should_fail_via_assert != true"
    
        - name: Assertions can have contain conditions.
          assert:
            that:
              - should_fail_via_fail != true
              - should_fail_via_assert != true
              - should_fail_via_complex_assert != true

Switch the boolean values of `should_fail_via_fail`, `should_fail_via_assert`, and `should_fail_via_complex_assert` to trigger each of the three `fail`/`assert` tasks, to see how they work.

For most test cases, `debug`, `fail`, and `assert` are all you need to ensure your infrastructure is in the correct state during a playbook run.

### Checking syntax and performing dry runs

Two of the simplest checks you should include in an automated playbook testing workflow are `--syntax-check` (which checks the playbook syntax to find simple quoting, formatting, or whitespace errors) and `--check` (which will run your entire playbook in `check` mode.

Syntax checking is extremely straightforward, and only requires a few seconds for even larger, more complex playbooks with dozens or hundreds of includes. You should include an `ansible-playbook my-playbook.yml --syntax-check` in any kind of CI that you do, and it's good practice to run a syntax check right before you check in playbook changes in your version control system.

Running a playbook in `check` mode is a little bit more involved, as Ansible will actually run the entire playbook on your live infrastructure, but in a way that results in no changes. Instead, it will mark any tasks that _would've_ resulted in a change so you can see what will happen when you _actually_ run the playbook later.

This can be helpful for two purposes:

  1. To prevent 'configuration drift', where a server might have some configuration that's drifted away from your coded configuration. This could happen due to human intervention (though hopefully you don't regularly log into your servers and make changes by hand anymore!) or due to other factors. But it's good to know what configuration is incorrect without necessarily changing it.
  2. To make sure changes you make to a playbook that shouldn't break idempotency _don't_, in fact, break idempotency. For example, if you're making a change to build a configuration file differently, but with the same resulting file, running the playbook with `--check` can alert you to the fact that you might accidentally end up changing the file as a result of your change. Time to fix your playbook!

When using `--check` mode, there are sometimes tasks that you still want to run (e.g. `command` tasks that register variables used in later tasks). You can make sure those tasks are _always_ run using the `always_run` option:

{lang=text,linenos=off}
    - name: A task that runs all the time, even in check mode.
      command: mytask --option1 --option2
      register: my_var
      always_run: true

Finally, for even more detailed information about what changes would occur, you can add the `--diff` option, and Ansible will output all the line-by-line changes that _would've_ been made to your servers. This option will produce a lot of output if `check` mode makes a lot of changes, so it's best to use this option conservatively, unless you want to scroll through a lot of text!

In addition to Ansible's robust but simple `--check-syntax` and `--check` modes, you might be interested in also running [Ansible Lint](https://github.com/willthames/ansible-lint) on your playbooks. Ansible Lint allows you to check for deprecated syntax or inefficient task structures, and is highly configurable so you can set up the linting to follow the playbook standards you and your team choose.

### Automated testing on GitHub using Travis CI

Automated testing using a continuous integration tool like Travis CI (which is free for public projects and integrated very well with GitHub) allows you to run tests against Ansible playbooks or roles you have hosted on GitHub with every commit.

There are four main things that should be tested when building and maintaining Ansible playbooks or roles:

  1. The playbook or role's syntax (are all the .yml files formatted correctly?).
  2. Whether the playbook or role will run through all the included tasks without failing.
  3. The playbook or role's idempotence (if run again, it should not make any changes!).
  4. The playbook or role's success (does the role do what it should be doing?).

Ultimately, the most important aspect is #4, because what's the point of a playbook or role if it doesn't do what you want it to do (e.g. start a web server, configure a database, deploy an app, etc.)?

We're going to assume, for the rest of this example, that you're testing a role you have on GitHub, though the example can be applied just as easily for standalone Ansible playbooks.

#### Setting up a role for testing

Since you're going to need a simple Ansible playbook and inventory file to test your role, you can create both inside a new 'tests' directory in your Ansible role:

{lang=text}
    # Directory structure:
    my_role/
      tests/
        test.yml <-- your test playbook
        inventory <-- an inventory file to use with the playbook

Inside the inventory file, add:

{lang=text}
    localhost

We just want to tell Ansible to run commands on the local machine (we'll use the --connection=local option when running the test playbook).

Inside `test.yml`, add:

{lang=text}
    ---
    - hosts: localhost
      remote_user: root
      roles:
        - github-role-project-name

Substitude your own role name for `github-role-project-name` (e.g. `ansible-role-django`). This is a typical Ansible playbook, and we tell Ansible to run the tasks on localhost, with the `root` user (otherwise, you could run tasks with `travis` if you want, and use `sudo` on certain tasks). You can add `vars`, `vars_files`, etc. if you want, but we'll keep things simple, because for many smaller roles, the role is pre-packaged with sane defaults and all the other info it needs to run.

The next step is to add a `.travis.yml` file to your role so Travis CI will pick it up and use it for testing. Add that file to the root level of your role, and add the following to kick things off:

{lang=yaml}
    ---
    language: python
    python: "2.7"
    
    before_install:
      # Make sure everything's up to date.
      - sudo apt-get update -qq
    
    install:
      # Install Ansible.
      - pip install ansible
    
      # Add ansible.cfg to pick up roles path.
      - "printf '[defaults]\nroles_path = ../' > ansible.cfg"
    
    script:
      # We'll add some commands to test the role here.

The only surprising part here is the `printf` line in the `install` section; I've added that line to create a quick and dirty `ansible.cfg` configuration file Ansible will use to set the `roles_path` one directory up from the current working directory. That way, we can include roles like `github-role-project-name`, or if we use `ansible-galaxy` to download dependencies (as another command in the install section), we can just use `- galaxy-role-name-here` to include that role in our `test.yml` playbook.

Now that we have the basic structure, it's time to start adding the commands to test our role.

#### Testing the role's syntax

This is the easiest test; `ansible-playbook` has a built in command that will check a playbook's syntax (including all the included files and roles), and return `0` if there are no problems, or an error code and some output if there were any syntax issues.

{lang=text}
    ansible-playbook -i tests/inventory tests/test.yml --syntax-check

Add this as a command in the `script` section of `.travis.yml`:

{lang=text}
    script:
      # Check the role/playbook's syntax.
      - ansible-playbook -i tests/inventory tests/test.yml --syntax-check

If there are any syntax errors, Travis will fail the build and output the errors in the log.

#### Role success - first run

The next aspect to check is whether the role runs correctly or fails on its first run.

{lang=text}
    # Run the role/playbook with ansible-playbook.
    - "ansible-playbook -i tests/inventory tests/test.yml --connection=local --sudo"

This is a basic ansible-playbook command, which runs the playbook `test.yml` against the local host, using `--sudo`, and with the inventory file we added to the role's `tests` directory.

Ansible returns a non-zero exit code if the playbook run fails, so Travis will know whether the command succeeded or failed.

#### Role idempotence

Another important test is the idempotence test---does the role change anything if it runs a second time? It should not, since all tasks you perform via Ansible should be idempotent (ensuring a static/unchanging configuration on subsequent runs with the same settings).

{lang=text}
    # Run the role/playbook again, checking to make sure it's idempotent.
    - >
      ansible-playbook -i tests/inventory tests/test.yml --connection=local --sudo
      | grep -q 'changed=0.*failed=0'
      && (echo 'Idempotence test: pass' && exit 0)
      || (echo 'Idempotence test: fail' && exit 1)

This command runs the exact same command as before, but pipes the results through grep, which checks to make sure 'changed' and 'failed' both report `0`. If there were no changes or failures, the idempotence test passes (and Travis sees the `0` exit and is happy), but if there were any changes or failures, the test fails (and Travis sees the `1` exit and reports a build failure).

#### Role success - final result

The last thing I check is whether the role actually did what it was supposed to do. If it configured a web server, is the server responding on port 80 or 443 without any errors? If it configured a command line application, does that command line application work when invoked, and do the things it's supposed to do?

{lang=text}
    # Request a page via the web server, to make sure it's running and responds.
    - "curl http://localhost/"

In this example, I'm testing a web server by loading 'localhost'; curl will exit with a 0 status (and dump the output of the web server's response) if the server responds with a 200 OK status, or will exit with a non-zero status if the server responds with an error status (like 500) or is unavailable.

Taking this a step further, you could even run a deployed application or service's own automated tests after ansible is finished with the deployment, thus testing your infrastructure and application in one go---but we're getting ahead of ourselves here... that's a topic for later!

#### Some notes about Travis CI

There are a few things you need to know about Travis CI, especially if you're testing Ansible, which will rely heavily on the VM environment inside which it is running:

  - **Ubuntu 12.04**: As of this writing, the only OS available via Travis CI is Ubuntu 12.04. Most of my roles work with Ubuntu/Debian/RedHat/CentOS, so it's not an issue for me... but if your roles strictly target a non-Debian-flavored distro, you probably won't get much mileage out of Travis. (There is an [open issue](https://github.com/travis-ci/travis-ci/issues/2046) to get Travis upgraded to Ubuntu 14.04, at least).
  - **Preinstalled packages**: Travis CI comes with a bunch of services installed out of the box, like MySQL, Elasticsearch, Ruby, etc. In the `.travis.yml` `before_install` section, you may need to do some `apt-get remove --purge [package]` commands and/or other cleanup commands to make sure the VM is fresh for your Ansible role's run.
  - **Networking/Disk/Memory**: Travis CI continously shifts the VM specs you're using, so don't assume you'll have X amount of RAM, disk space, or network capacity. You can add commands like `cat /proc/cpuinfo`, `cat /proc/meminfo`, `free -m`, etc. in the `.travis.yml` `before_install` section if you need to figure out the resources available in your VM.

See much more information about the VM environment on the [Travis CI Build Environment page](http://docs.travis-ci.com/user/ci-environment/).

#### Real-world examples

This style of testing is integrated into many of the `geerlingguy.*` roles on Ansible Galaxy; here are a few example roles that use Travis CI integration in the way outlined above:

  - https://github.com/geerlingguy/ansible-role-apache
  - https://github.com/geerlingguy/ansible-role-gitlab
  - https://github.com/geerlingguy/ansible-role-mysql

If you would like to run your tests using a slightly more simplified and more self-contained test running environment, you might be interested in the [rolespec](https://github.com/nickjj/rolespec) project, which formalizes and simplifies much of the code in the Travis CI examples shown above. Your tests will be dependent on another library for test runs, but doing this allows you to run the tests more easily in environments other than Travis CI.

### Functional testing using serverspec

[Serverspec](http://serverspec.org/) is a tool to help automate server tests using RSpec tests, which use a Ruby-like DSL to ensure your server configuration matches your expectations. In a sense, it's another way of building well-tested infrastructure.

Serverspec tests can be run locally, via SSH, through Docker's APIs, or through other means, without the need for an agent installed on your servers, so it's a lightweight tool for testing your infrastructure (just like Ansible is a lightweight tool for _managing_ your infrastructure).

There's a lot of debate over whether well-written Ansible playbooks themselves (especially along with the dry-run `--check` mode) are adequate for well-tested infrastructure, but many teams are more comfortable maintaining infrastructure tests in Serverspec instead (especially if the team is already familiar with how Serverspec and Rspec works!).

Consider this: a truly idempotent Ansible playbook is already a great testing tool if it uses Ansible's robust core modules and `fail`, `assert`, `wait_for` and other tests to ensure a specific state for your server. If you use Ansible's `user` module to ensure a given user exists and is in a given group, and run the same playbook with `--check` and get `ok` for that same task, isn't that a good enough test that your server is configured correctly?

This book will not provide a detailed guide for using Serverspec with your Ansible-managed servers, but here are a few resources in case you'd like to use it:

  - [A brief introduction to server testing with Serverspec](https://www.debian-administration.org/article/703/A_brief_introduction_to_server-testing_with_serverspec)
  - [Testing Ansible Roles with Test Kitchen, Serverspec and RSpec](http://www.slideshare.net/MartinEtmajer/testing-ansible-roles-with-test-kitchen-serverspec-and-rspec-48185017)
  - [Testing infrastructure with serverspec](http://vincent.bernat.im/en/blog/2014-serverspec-test-infrastructure.html)

## Summary

Tools to help manage, test, and run playbooks regularly and easily, such as Travis CI, Jenkins, and Ansible Tower, also help deliver certainty when applying changes to your infrastructure using Ansible. In addition the information contained in this chapter, read through the [Testing Strategies](http://docs.ansible.com/test_strategies.html) documentation in Ansible's documentation for a comprehensive overview of infrastructure testing and Ansible.

{lang="text",linenos=off}
     ________________________________________
    / The first rule of any technology used  \
    | in a business is that automation       |
    | applied to an efficient operation will |
    | magnify the efficiency. The second is  |
    | that automation applied to an          |
    | inefficient operation will magnify the |
    \ inefficiency. (Bill Gates)             /
     ----------------------------------------
            \   ^__^
             \  (oo)\_______
                (__)\       )\/\
                    ||----w |
                    ||     ||
